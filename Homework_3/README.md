# Проекция изображений с использованием StyleGAN2-ADA-PyTorch

В этом документе рассмотрены два подхода к проекции изображений с использованием библиотеки StyleGAN2-ADA-PyTorch. Для каждого из подходов приведены описания изменений в коде и объяснения их влияния на процесс проекции.

## Попытка 1: Начальный подход к проекции изображений
### Изменения в коде и их влияние
Переменная w_opt и обновление латентных переменных

- Внесена возможность обновления латентных переменных (requires_grad=True), что позволило оптимизировать латентные переменные для лучшего совпадения с целевым изображением.

- Добавлен sceduler

# Итоги на даном этапе: 
![images_table](https://github.com/Enc0der/Deep_gen_models/assets/70717995/00baec00-b2f3-4743-b86c-e8ad1fbc52fd)
## Попытка 2: Улучшенный подход к проекции изображений

### Изменения в коде и их влияние

Добавление модуля time и функции denormalize позволило сохранять изображения с уникальными именами и корректным отображением цветов.

# Итоги на данном этапе: 

![Изображение 3](artifacts/new_al_DK!.jpg)
![Изображение 3](artifacts/new_al_DK.jpg)
![Изображение 3](artifacts/new_al_J.jpg)
![Изображение 3](artifacts/new_al_J1.jpg)
![Изображение 3](artifacts/new_al_K.jpg)
![Изображение 3](artifacts/new_al_K1.jpg)
![Изображение 3](artifacts/new_al_M.jpg)
![Изображение 3](artifacts/new_al_M1.jpg)
![Изображение 3](artifacts/new_al_S.jpg)
![Изображение 3](artifacts/new_al_S1.jpg)

### Заключение

Обновленный код предлагает улучшения для сохранения результатов проекции, предотвращая перезапись и обеспечивая корректное отображение цветов.

## Стили


### Пояснения по использованию индексов из вектора W+ при смешивании стилей

Для смешивания стилей в векторе W+ StyleGAN2-ADA-PyTorch используются индексы, которые указывают на конкретные слои вектора W+. В данном контексте вектор W+ представляет собой последовательность векторов латентного пространства для каждого слоя генератора.

1. **Загрузка латентного кода базового изображения:**
```python
base_image_path = "/content/projected_images_2/AJoly/0_latent.pt"
base_latent = torch.load(base_image_path)
```

2. **Расширение латентного вектора:**
```python
base_latent_expanded = expand_latent_vector(base_latent)
```
Этот шаг расширяет латентный вектор базового изображения до необходимой формы для использования в генераторе.

3. **Расширение латентных векторов стилевых изображений:**
```python
style_latent_expanded = [expand_latent_vector(torch.load(path)) for path in style_latent_paths]
```
Аналогично базовому вектору, латентные векторы стилевых изображений также расширяются до необходимой формы.

4. **Индексы для стилевого смешивания:**
```python
style_indices = [4, 8, 12]
```
Эти индексы указывают на конкретные слои вектора W+, которые будут использоваться для смешивания стилей. Например, 4, 8 и 12 могут соответствовать различным аспектам стиля, таким как форма лица, текстура кожи и цвет волос.

5. **Смешивание латентных векторов:**
```python
mixed_latent = mix_latents(base_latent_corrected, expanded_style_latent, style_indices)
```
Эта функция смешивает базовый латентный вектор с расширенным латентным вектором стилевого изображения, используя указанные индексы для определения частей вектора, которые будут изменены.

6. **Генерация и сохранение изображения:**
```python
mixed_image = G.synthesis(mixed_latent.to(device))
save_image(denormalize(mixed_image.clamp(-1, 1)), mixed_image_path)
```
Здесь смешанный латентный вектор используется для синтеза изображения с помощью генератора. Затем полученное изображение сохраняется для последующего отображения или использования.

Таким образом, индексы из вектора W+ указывают на конкретные характеристики стиля, которые мы хотим изменить при смешивании, позволяя контролировать, какие аспекты стиля будут унаследованы от каждого изображения.

### Итоги 1 обработки:

![Изображение 3](artifacts/style1.png)
![Изображение 3](artifacts/style12.png)
![Изображение 3](artifacts/style13.png)

Видно, что стили смешиваются на более высоких уровнях векторов, что не позволяет поменять оригиналу цвет лица.

## Попытка 2 
В этом фрагменте кода были внесены следующие изменения:

1. **Изменение индексов для стилевого смешивания:**
```python
style_indices = list(range(8, 18))
```
Вместо явного указания конкретных индексов для смешивания стилей, здесь используется функция `range()`, чтобы автоматически создать список индексов, начиная с 8-го слоя и заканчивая последним (включительно).

2. **Функция для смешивания латентных кодов:**
```python
def mix_latents(base_latent, style_latent, style_indices):
    # Код функции mix_latents остался без изменений
```
Сама функция для смешивания латентных кодов осталась без изменений.

3. **Смешивание стилевых латентных кодов с базовым латентным кодом:**
```python
for i, expanded_style_latent in enumerate(expanded_style_latents):
    # Код для смешивания стилевых латентных кодов с базовым остался без изменений
```
Цикл по-прежнему выполняет смешивание всех стилевых латентных кодов с базовым латентным кодом, используя функцию `mix_latents`.

4. **Сохранение и отображение изображения:**
```python
mixed_image_normalized = denormalize(mixed_image.clamp(-1, 1))
save_image(mixed_image_normalized, mixed_image_path)
display(Image.open(mixed_image_path))
```
Процесс сохранения и отображения смешанного изображения остался без изменений. Однако, в данном контексте предполагается, что изображения сохраняются и отображаются в цикле, в зависимости от количества стилевых латентных кодов.


![Изображение 1](artifacts/new_alegned_blue.jpg)
![Изображение 3](artifacts/style23.png)

![Изображение 2](artifacts/new_aligned_green.jpg)
![Изображение 2.1](artifacts/style2.png)

![Изображение 3](artifacts/new_al_style.jpg)
![Изображение 1.1](artifacts/style22.png)




# Улыбки для актеров: 

# Добавление улыбки к лицам на фотографиях с использованием StyleGAN

Проект позволяет добавлять улыбку к лицам на фотографиях, используя технологию StyleGAN для модификации латентных векторов.

## Основная идея

С помощью интерполяции между латентным вектором исходного изображения и латентным вектором, представляющим улыбку, достигается добавление улыбки к лицу на фотографии. Этот процесс включает загрузку соответствующих латентных векторов, их интерполяцию и последующую генерацию изображения с измененным выражением лица.

## Процесс работы

1. **Загрузка латентных векторов**: Исходный латентный вектор, представляющий лицо без улыбки, и латентный вектор улыбки загружаются из сохраненных файлов.

2. **Интерполяция латентных векторов**: Функция `interpolate` изменяет исходный латентный вектор, включая в него элементы вектора улыбки. Параметр `psi` контролирует степень влияния вектора улыбки, а параметр `indeces` определяет, какие слои латентного пространства будут изменены.

3. **Генерация изображений**: Используется модель StyleGAN для генерации нового изображения на основе модифицированного латентного вектора. Полученное изображение демонстрирует, как лицо на фотографии приобретает улыбку.

4. **Визуализация результатов**: В коде предусмотрена возможность сравнения исходного изображения и изображения с добавленной улыбкой, что позволяет наглядно оценить результаты изменений.


![Изображение 3](artifacts/smile1.png)
![Изображение 3](artifacts/smile2.png)
![Изображение 3](artifacts/smile3.png)
![Изображение 3](artifacts/smile4.png)
![Изображение 3](artifacts/smile5.png)
![Изображение 3](artifacts/smile6.png)
![Изображение 3](artifacts/smile7.png)
![Изображение 3](artifacts/smile8.png)
![Изображение 3](artifacts/smile9.png)
![Изображение 3](artifacts/smile10.png)

# Пересадка лиц 

# Эксперименты с функциями потерь и оптимизацией при переносе лица

В рамках данного проекта проводились эксперименты по переносу лиц между различными изображениями, используя архитектуру StyleGAN и различные функции потерь для достижения максимально реалистичных результатов.

## Основные компоненты эксперимента

- **Модификация функций потерь**: Использовались две ключевые функции потерь: LPIPS (Learned Perceptual Image Patch Similarity) для оценки перцептивного расстояния между изображениями и ArcFace Loss для сохранения идентичности лиц на переносимых изображениях.

- **Адаптация изображений**: Все изображения были приведены к единому размеру и нормализованы перед подачей в модель, что обеспечивало универсальность и сопоставимость результатов.

- **Интерполяция латентных пространств**: Эксперименты включали интерполяцию между латентными векторами исходного и целевого изображений для плавного переноса характеристик лица.

## Настройка параметров эксперимента

Для улучшения качества переноса лица экспериментировались с различными настройками:

- **Варьирование весов функций потерь**: Изменялся баланс между LPIPS и ArcFace Loss, что позволяло находить оптимальное соотношение между сохранением идентичности лица и общим перцептивным качеством изображения.

- **Изменение параметров интерполяции**: Регулировалась степень влияния целевого латентного вектора на исходный, что давало возможность контролировать интенсивность переносимых изменений.

## Выводы

Эксперименты показали значительное влияние настройки функций потерь и параметров интерполяции на качество и реалистичность результатов. Наиболее удачные комбинации параметров позволяли достигать высокой степени схожести переносимого лица с целевым, сохраняя при этом естественность выражений и особенностей лица.

## Таблица с итоговым результатом

![meow](artifacts/4A381CAF-3B8D-4DD4-9398-D69889622DDD.jpeg)


